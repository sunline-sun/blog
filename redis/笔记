
Why GitHub? 
Team
Enterprise
Explore 
Marketplace
Pricing 

Sign in
Sign up
JingRui-H
/
blog
 Watch 1  Star 0  Fork 0
Code
Issues
Pull requests
Actions
Projects
Security
Insights
 master 
blog/redis/TEMP.md
Go to file

@JingRui-H
JingRui-H Update TEMP.md
Latest commit c595621 on 5 Jan
 History
 1 contributor
967 lines (701 sloc)  58.1 KB
RawBlame
  
redis是单线程吗?

Redis是单线程，主要是指Redis的网络IO和键值对读写是由一个线程来完成的，这也是Redis对外提供键值存储服务的主要流程。 但Redis的其他功能，比如持久化、异步删除、集群数据同步等，其实是由额外的线程执行的。

那为什么不用多线程呢? 因为多线程面临共享资源并发控制的问题, 比如两个线程一起去操作list的lpush/lpop, 那么就变成了粗粒度串行. 如果要细粒度串行就需要处理很多线程安全问题.

性能瓶颈: 因为单线程, 所以任何耗时操作都算. 比如bigkey/全量返回等

那为什么redis的单线程模式这么快?

redis大部分操作都是内存上完成的, 再加上采用了高效的数据结构, 比如哈希表和跳表
多路复用机制
Redis单线程处理IO请求性能瓶颈主要包括2个方面：

1、任意一个请求在server中一旦发生耗时，都会影响整个server的性能，也就是说后面的请求都要等前面这个耗时请求处理完成，自己才能被处理到。耗时的操作包括以下几种： a、操作bigkey：写入一个bigkey在分配内存时需要消耗更多的时间，同样，删除bigkey释放内存同样会产生耗时； b、使用复杂度过高的命令：例如SORT/SUNION/ZUNIONSTORE，或者O(N)命令，但是N很大，例如lrange key 0 -1一次查询全量数据； c、大量key集中过期：Redis的过期机制也是在主线程中执行的，大量key集中过期会导致处理一个请求时，耗时都在删除过期key，耗时变长； d、淘汰策略：淘汰策略也是在主线程执行的，当内存超过Redis内存上限后，每次写入都需要淘汰一些key，也会造成耗时变长； e、AOF刷盘开启always机制：每次写入都需要把这个操作刷到磁盘，写磁盘的速度远比写内存慢，会拖慢Redis的性能； f、主从全量同步生成RDB：虽然采用fork子进程生成数据快照，但fork这一瞬间也是会阻塞整个线程的，实例越大，阻塞时间越久； 2、并发量非常大时，单线程读写客户端IO数据存在性能瓶颈，虽然采用IO多路复用机制，但是读写客户端数据依旧是同步IO，只能单线程依次读取客户端的数据，无法利用到CPU多核。

针对问题1，一方面需要业务人员去规避，一方面Redis在4.0推出了lazy-free机制，把bigkey释放内存的耗时操作放在了异步线程中执行，降低对主线程的影响。

针对问题2，Redis在6.0推出了多线程，可以在高并发场景下利用CPU多核多线程读写客户端数据，进一步提升server性能，当然，只是针对客户端的读写是并行的，每个命令的真正操作依旧是单线程的

多路复用

我们经常用的IO是BIO(blocking I/O), 在Java中的NIO(非阻塞I/O, New I/O) 底层是通过多路复用I/O模型实现的. 而现实的场景也是诸如netty，redis，nginx，nodejs都是采用的多路复用I/O模型.

多路: 多个客户端连接（连接就是套接字描述符）
复用: 使用单进程就能够实现同时处理多个客户端的连接
其发展可以分select->poll->epoll三个阶段来描述.

select就是轮询，在Linux上限制个数一般为1024个

poll解决了select的个数限制，但是依然是轮询

epoll解决了个数的限制，同时解决了轮询的方式

select函数监视的文件描述符分3类，分别是writefds、readfds、和exceptfds。调用后select函数会阻塞，直到有描述符就绪（有数据可读、可写、或者有except），或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以通过遍历fdset，来找到就绪的描述符。 select具有良好的跨平台支持，其缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024

poll改变了文件描述符集合的描述方式，使用了pollfd结构而不是select的fd_set结构，使得poll支持的文件描述符集合限制远大于select的1024

下图就是基于多路复用的 Redis IO 模型。图中的多个 FD 就是刚才所说的多个套接字。Redis 网络框架调用 epoll 机制，让内核监听这些套接字。此时，Redis 线程不会阻塞在某一个特定的监听或已连接套接字上，也就是说，不会阻塞在某一个特定的客户端请求处理上。正因为此，Redis 可以同时和多个客户端连接并处理请求，从而提升并发性。 !image

为了在请求到达时能通知到 Redis 线程，select/epoll 提供了基于事件的回调机制，即针对不同事件的发生，调用相应的处理函数。那么，回调机制是怎么工作的呢？其实，select/epoll 一旦监测到 FD 上有请求到达时，就会触发相应的事件。这些事件会被放进一个事件队列，Redis 单线程对该事件队列不断进行处理。这样一来，Redis 无需一直轮询是否有请求实际发生，这就可以避免造成 CPU 资源浪费。同时，Redis 在对事件队列中的事件进行处理时，会调用相应的处理函数，这就实现了基于事件的回调。因为 Redis 一直在对事件队列进行处理，所以能及时响应客户端请求，提升 Redis 的响应性能。

持久化

AOF(Append Only File)

比如set testkey testvalue

AOF文件

*3  //当前命令有三个部分
$3  //一共三个字节
set
$7
testkey
$9
testvalue
先执行,之后记录log. 好处:

为了避免检查开销, 如果先执行日志的话可能会记录错误日志.
执行完之后记录不会阻塞当前的写操作
缺点:

如果执行完还没记日志就直接宕机, 日志就丢失了
可能会给下一个操作带来阻塞风险, 因为aof日志也是在主线程中执行的
回写策略

这里的缺点其实都和回写策略有关, aof配置项appendfsync的三个可选值

同步写回(Always): 每个写命令执行完, 立马同步地将日志写回磁盘
每秒写回(Everysec默认): 每个写命令执行完, 只是先把日志写到 AOF文件的内存缓冲区, 每隔一秒把缓冲区中的内容写入磁盘
操作系统控制的写回(No): 每个写命令执行完, 只是先把日志写到AOF文件的内存缓冲区, 由操作系统决定何时将缓冲区内容写回磁盘. 性能最高
重写机制

aof会追加到越来越大, 就会有各种风险

文件系统本身对文件大小有限制
文件太大, 追加命令效率会变慢
如果宕机需要恢复, 日志很大恢复很慢
Redis 根据数据库的现状创建一个新的AOF文件，也就是说，读取数据库中的所有键值对，然后对每一个键值对用一条命令记录它的写入

重写过程:

主线程fork出后台bgrewriteaof子进程. fork子进程时，子进程是会拷贝父进程的页表，即虚实映射关系，而不会拷贝物理内存。子进程复制了父进程页表，也能共享访问父进程的内存数据了，此时，类似于有了父进程的所有内存数据
bgrewriteaof子进程就可以在不影响主线程的情况下，逐一把拷贝的数据写成操作，记入重写日志。
如果有新的操作进来, 会写入正在使用的aof文件中, 同时写日志到AOF重写缓冲中，等从内存拷贝的AOF重写日志生成好后再合并进去，生成完整的AOF重写日志
新的aof代替旧的
风险点: fork可能阻塞主线程, fork操作执行时，内核需要给子进程拷贝主线程的页表。如果主线程的内存大，页表也相应大，拷贝页表耗时长，会阻塞主线程。 如果重写的时候来了写入或更新bigkey, 那么主线程需要申请内存空间来保存这个数据, 可能会阻塞主线程.

RDB

问题:

对那些数据做快找
快照时, 会阻塞主线程吗
redis提供了两个命令来生成RDB文件

save：在主线程中执行，会导致阻塞
bgsave：创建一个子进程，专门用于写入RDB文件，避免了主线程的阻塞，这也是Redis RDB文件生成的默认配置。
自动触发:

save 900 1 # 表示900 秒内如果至少有 1 个 key 的值变化，则触发RDB
save 300 10 # 表示300 秒内如果至少有 10 个 key 的值变化，则触发RDB
save 60 10000 # 表示60 秒内如果至少有 10000 个 key 的值变化，则触发RDB
生成RDB过程:

主线程fork生成bgsave子进程, 可以共享主线程的所有内存数据
bgsave子进程运行后，开始读取主线程的内存数据，并把它们写入 RDB文件
(写时复制)如果主线程此时有写入操作, 直接写入主线程. 主线程会把新数据或修改后的数据写到一个新的物理内存地址上，并修改主线程自己的页表映射. 所以，子进程读到的类似于原始数据的一个副本，而主线程也可以正常进行修改
多久快照一次呢? 在Redis 4.0中提出了一个混合使用AOF日志和内存快照的方法, 设置的参数是: aof-use-rdb-preamble yes

这样一来，快照不用很频繁地执行，这就避免了频繁fork对主线程的影响。而且，AOF日志也只用记录两次快照间的操作，也就是说，不需要记录所有操作了，因此，就不会出现文件过大的情况了，也可以避免重写开销。

建议:

数据不能丢失时，内存快照和AOF的混合使用是一个很好的选择
如果允许分钟级别的数据丢失，可以只使用 RDB
如果只用AOF，优先使用everysec的配置选项，因为它在可靠性和性能之间取了一个平衡
主从库如何实现数据一致

比如实例1（ip：172.16.19.3）和实例2（ip：172.16.19.5）我们在实例2上执行replicaof 172.16.19.3 6379，实例2就变成了实例1的从库，并从实例1上复制数据. Redis5.0之前使用slaveof

流程:

从库给主库发送psync ? -1命令. 其中?表示runID, 是每个redis实例启动都会自动生成的一个随机ID, 标识唯一. 因为不知道主库runID, 所以?. -1表示offset, 第一次复制为-1
主库响应从库FULLRESYNC [runID] [offset]返回给从库, 从库收到会记录这两个参数. 注意: 这里第一次复制是全量复制
主库执行bgsave, 将RDB文件发送给从库. 从库收到RDB之后会清空本地数据, 再加载RDB文件. 如果同步过程主库会有写操作, 会记录在replication buffer中
主库会把replication buffer记录的发送给从库
如果有多个从库怎么处理, 生成RDB和传输RDB很占用资源? 最好是主 -> 从 -> 从

那如果网络断了怎么处理数据同步呢? 在Redis 2.8之前是直接重新全量复制, 之后就是用增量复制方式同步.

当从库断连又重连之后，通过psync命令告诉主库自己的slave_repl_offset，然后主库根据自己的master_repl_offset和slave_repl_offset来判断是需要全量同步还是把两者之间的命令增量同步给从库（同步的方式就是通过主库与每个从库建立连接之后的这个所谓的replication buffer）

repl_backlog_buffer是一个环形缓冲区, 主库会记录自己偏移量master_repl_offset, 从库会记录自己的偏移量slave_repl_offset. 所以在缓冲区写满后，主库会继续写入，此时，就会覆盖掉之前写入的操作。一个从库如果和主库断连时间过长，造成它在主库repl_backlog_buffer的slave_repl_offset位置上的数据已经被覆盖掉了，此时从库和主库间将进行全量复制。

如果不想全量复制, 可以设置repl_backlog_buffer大一点.

建议redis实例不要太大, 一般在几GB比较合适, 减少RDB生成传输和重新加载的开销.

哨兵机制

三大问题:

主库真的挂了吗？
该选择哪个从库作为主库？
怎么把新主库的相关信息通知给从库和客户端呢？
哨兵其实就是一个运行在特殊模式下的 Redis 进程，主从库实例运行的同时，它也在运行。哨兵主要负责的就是三个任务：监控、选主（选择主库）和通知

监控: 周期性PING主从, 如果从没在规定时间响应标记为主观下线. 如果主没有在规定时间内响应, 先标记为主观下线. 然后哨兵集群的其他哨兵一起判断看看是不是主观下线, 如果大多数都是主观下线就会被标记为客观下线. 开始主从切换

选主:

先使用配置项down-after-milliseconds(表示主节点没有连接上从节点的最大时间), 如果断线次数超过10次, 就把不符合的从库筛选掉.
给剩余从库打分, 先根据从库优先级, 如果有优先级最高的, 直接变主库. 如果一样, 继续判断主从同步进度, 看从库偏移量slave_repl_offset哪个和主库偏移量master_repl_offset最接近. 如果还是有一样的, 就看从库ID最小的, ID是实例唯一标识
通知: 把新主库连接信息发给从库, 执行replicaof命令, 和新主库建立连接和复制. 同时会把新主库连接信息发送给客户端

如果哨兵挂了, 主从切换就不能用了. 这里就开始哨兵集群了. 如果你部署过哨兵集群的话就会知道，在配置哨兵的信息时，我们只需要用到下面的这个配置项，设置主库的 IP 和端口，并没有配置其他哨兵的连接信息 sentinel monitor <master-name> <ip> <redis-port> <quorum>, 哨兵是基于pub/sub机制的哨兵集群组成, 通过Redis进行消息的发布和订阅, 在主从集群中，主库上有一个名为__sentinel__:hello的频道，不同哨兵就是通过它来相互发现，实现互相通信的. 哨兵又给主库发送INFO命令, 获得从库的连接信息, 就可以和从库建立连接进行监控了.

由哪个哨兵去切换主从呢? 哨兵选举TODO  总结：redis的投票机制    投票过程是集群中所有master参与,如果半数以上master节点与master节点通信超时(cluster-node-timeout),认为当前master节点挂掉。    选举的依据依次是：网络连接正常->5秒内回复过INFO命令->10*down-after-milliseconds内与主连接过的->从服务器优先级->复制偏移量->运行id较小的。选出之后通过slaveif no ont将该从服务器升为新主服务器。    通过slaveof ip port命令让其他从服务器复制该信主服务器。    最后当旧主重新连接后将其变为新主的从服务器

选举的依据依次是：网络连接正常->5秒内回复过INFO命令->10*down-after-milliseconds内与主连接过的->从服务器优先级->复制偏移量->运行id较小的

切片集群

如果数据量很大, 使用RDB持久化的时候, fork是很慢的. 可以使用INFO命令查看latest_fork_usec(微妙). 这时候有一个比较好的解决方案, 就是切片集群/分片集群

这里官方在Redis3.0之后就出了Redis Cluster作为切片集群的解决方案.

数据分布

Redis Cluster采用哈希槽(Hash Slot)来处理, 一个切片集群共有16384个哈希槽.

先是根据键值对的key按照CRC16算法计算出一个16bit的值, 再去对于16384取模, 就知道映射在哪一个哈希槽
在使用cluster create创建集群时, redis会自动把这些槽分布在集群实例上. 当然也可以使用cluster meet手动建立实例间的连接, 形成集群, 在使用cluster addslot指定每个实例上的哈希槽个数(手动分配哈希槽总数一定要为16384)
客户端定位数据

数据所处的哈希槽是可以计算获得的, 这个计算是由客户端在发送请求时来执行的. 客户端和集群建立连接后, 实例就会把所有哈希槽分配信息发送给客户端. 但是开始的时候实例只知道自己分配的哈希槽, 因为Redis实例会把自己的哈希槽信息发给和它相连接的其它实例, 来完成哈希槽分配信息的扩散.

客户端收到哈希槽信息后, 就会吧哈希槽信息缓存在本地. 然后就可以直接发送请求了. 但是实例对应哈希槽关系是会变化的:

集群实例新增或者删除
为了负载均衡, redis需要把哈希槽所有实例重新分布一遍
如果实例对应哈希槽关系变化了, 客户端并不知道. 发送给了错误的实例, 实例就会MOVED响应, 告诉新实例的访问地址. 客户端会重定向到正确的实例, 同时也会更新本地缓存

GET hello:key
(error) MOVED 13320 172.16.19.5:6379
如果slot1迁移到slot2只完成了一半, 这时候访问的是前移到了slot2的数据, 就不能返回move响应了. 因为这样后续还没迁移的就会直接请求到slot2. 这时候会响应(error) ASK 13320 172.16.19.5:6379, 表示哈希槽正在迁移, 目前请求的key在这个ip上, 然后客户端需要给slot2发送ASKING命令请求允许去获取数据, 接着在去发送GET请求获取数据. 这里是不会更新客户端缓存的

string

假设保存一亿个键值对, 用什么数据类型?

photo_id: 1101000051
photo_obj_id: 3301000051
用string会比较消耗内存, 因为string是sds类型, 还需要额外的数据保存数据长度, 空间使用等, 本身其实就保存了两个10位数的Long类型.

buf：字节数组，保存实际数据。为了表示字节数组的结束，Redis 会自动在数组最后加一个“\0”，这就会额外占用1个字节的开销。
len：占4个字节，表示buf的已用长度。
alloc：也占个4字节，表示buf的实际分配长度，一般大于len。
除了SDS占用, 还有就是redisObject结构体. 因为每一个key都会有一个RedisObject统一记录这些元数据, 一个RedisObject包含了8字节的元数据和一个8字节指针，这个指针再进一步指向具体数据类型的实际数据所在，例如指向String类型的SDS结构所在的内存地址.

还有就是因为一个个key是分散的, Redis会使用一个全局哈希表保存所有键值对，哈希表的每一项是一个dictEntry的结构体，用来指向一个键值对。dictEntry结构中有三个8字节的指针，分别指向key、value以及下一个dictEntry，三个指针共24字节.但是这三个指针只有24字节，为什么会占用了32字节呢？这就要提到Redis使用的内存分配库jemalloc了。jemalloc在分配内存时，会根据我们申请的字节数N，找一个比N大，但是最接近N的2的幂次数作为分配的空间，这样可以减少频繁分配的次数。所以会使用32字节.

统计

聚合统计

统计手机每天新用户和第二天留存用户数 用set记录用户信息, 然后使用统计命令类似 SUNIONSTORE user:id user:id user:id:20200803实现, 如果数据量很大建立在主从集群中用一个从库专门去复制Set 的差集、并集和交集的聚合计算

排序统计

可以使用list和zset, 但是list使用LRANGE test1 1 10获取分页时, 万一又有了一条新数据, 那么第二页数据就会重复一条数据, 被顶下来. 用zset的话使用ZRANGEBYSCORE test N-9 N, 按照权重自己分配, 就不会被顶下来了

二值状态统计

比如只记录是否的数据, 签到/未签到等, 可以使用bitmap. bitmap本身就是用string类型作为底层数据结构实现的一种二值状态统计. String类型是会保存为二进制的字节数组，所以Redis就把字节数组的每个bit位利用起来，用来表示一个元素的二值状态。你可以把Bitmap看作是一个bit数组。

Bitmap 提供了 GETBIT/SETBIT 操作，使用一个偏移值 offset 对 bit 数组的某一个 bit 位进行读和写。不过，需要注意的是，Bitmap 的偏移量是从 0 开始算的，也就是说 offset 的最小值是 0。当使用 SETBIT 对一个 bit 位进行写操作时，这个 bit 位会被设置为 1。Bitmap 还提供了 BITCOUNT 操作，用来统计这个 bit 数组中所有“1”的个数

//记录8月3号该用户签到
SETBIT uid:sign:3000:202008 2 1 
//检查该用户8月3日是否签到
GETBIT uid:sign:3000:202008 2 
//统计该用户在8月份签到次数
BITCOUNT uid:sign:3000:202008
统计连续签到8天可以把8天的用户的bitmap都做与操作, 为1的用户就是连续签到8天的用户

基数统计

统计网页uv, 需要去重, 一个用户一天内多次访问只算一次. 可以使用setSADD page1:uv user1. 如果page1非常火爆, uv达到千万, 一个set需要记录千万用户的ID, 有成千上万个这样的页面, 就会花费很多内存. 这时候可以使用hash记录HSET page1:uv user1 1, 但是页面很多也是很耗内存.

HyperLogLog就可以开始使用了, 它的最大优势就在于，当集合元素数量非常多时，它计算基数所需的空间总是固定的，而且还很小

在Redis中，每个HyperLogLog只需要花费12KB内存，就可以计算接近2^64个元素的基数。你看，和元素越多就越耗费内存的Set和Hash类型相比，HyperLogLog就非常节省空间。

//把访问页面的每个用户都添加到HyperLogLog中
PFADD page1:uv user1 user2 user3 user4 user5
//获得page1的UV值
PFCOUNT page1:uv
注意一下，HyperLogLog 的统计规则是基于概率完成的，所以它给出的统计结果是有一定误差的，标准误算率是 0.81%。这也就意味着，你使用 HyperLogLog 统计的 UV 是 100 万，但实际的 UV 可能是 101 万。虽然误差率不算大，但是，如果你需要精确统计结果的话，最好还是继续用 Set 或 Hash 类型

GEO

前面说到了Bitmap、HyperLogLog, GEO是用于位置信息服务（Location-Based Service，LBS）的应用. 需要记录用户的经纬度.

第一反应想到了hash去实现, key是用户的uid, value是经纬度. 但是我们还需要一个功能就是根据距离来排序, hash没有排序功能. 那就用sortedSet, key是地址, member是车辆ID, 权重是经纬度. 但是Sorted Set 元素的权重分数是一个浮点数（float 类型）, 而经纬度是两个值. 所以有了GEO

GeoHash编码方法

为了能高效地对经纬度进行比较，Redis 采用了业界广泛使用的 GeoHash 编码方法，这个方法的基本原理就是“二分区间，区间编码”。

计算的经纬度(116.37，39.86), 对于一个地理位置信息来说，它的经度范围是[-180,180], 现在经度是116.37, 然后进行五次分区

把最大的经度区间[-180,180]变成左分区[-180,0)和右分区[0,180]. 此时, 经度值116.37是属于右分区[0,180], 所以用1表示第一次二分区后的编码值
第二次[0,180], 分成[0,90)[90,180], 经度值116.37是属于右分区[90,180], 用1表示第一次二分区后的编码值
[90, 180] -> [90,135)[135,180], 属于左分区用0表示
[90,135] -> [90,112.5)[112.5,135], 属于右分区用1表示
[112.5,135] -> [112.5, 123.75)[123.75,135], 属于右分区用1表示
获得经度值的5位编码值, 即11010

只是纬度的范围是[-90，90], 现在纬度是39.86, 同理获得10111.

最后将经纬度各自编码值11010和10111, 各自取第一位, 在各自取第二位, 组合获得1110011101. 这里获得的GeoHash编码就可以作为权重分数了.

怎么操作GEO类型呢?

//假设车辆ID是33,经纬度位置是（116.034579，39.030452）,我们可以用一个GEO集合保存所有车辆的经纬度, 集合key是cars:locations
GEOADD cars:locations 116.034579 39.030452 33

//查找以这个经纬度为中心的5公里内的车辆信息
GEORADIUS cars:locations 116.054579 39.030452 5 km ASC COUNT 10
自定义数据类型

redis基本对象结构 RedisObject内部组成包括type、encoding、lru和refcount , 还有*ptr指针

type：表示值的类型，涵盖了我们前面学习的五大基本类型
encoding：是值的编码方式，用来表示 Redis 中实现各个基本类型的底层数据结构，例如 SDS、压缩列表、哈希表、跳表等；
lru：记录了这个对象最后一次被访问的时间，用于淘汰过期的键值对
refcount：记录了对象的引用计数
*ptr：是指向数据的指针。
只需定义新的数据类型还有type,coding之后用*ptr指针指向新类型的实现就好了.

物联网案例

需要周期性的统计近万台设备的实时状态, 包括设备ID、压力、温度、湿度，以及对应的时间戳

可查询单条记录, hash存key, 时间戳:温度等
可以对某个范围内数据查询, sortedSet存key, 时间戳:温度等
可以聚合计算, 使用基于RedisTimeSeries模块保存时间序列数据
作为消息队列

直接使用List作为消息队列, 生产者LPUSH, 消费者RPOP. 使用while(1)循环(解决顺序消费)
消费者使用BRPOP读取, 阻塞式读取, 没有数据时自动阻塞. 有新数据就开始读取
消费者使用BRPOPLPUSH, 消费之后再把消息放入另一个list, 保证消息安全性(解决消息可靠性)
冥等性在队列中加入唯一标识, 业务处理就好了(解决重复消费)
还有一个问题就是, 消费者只能单个消费, 我们希望是一个消费组消费. Redis从5.0版本开始提供的Streams数据类型了, 除了上面的功能, 还支持消费组形式的消息读取

命令:

XADD: 插入消息, 保证有序, 可以自动生成全局唯一ID
XREAD: 用于读取消息, 可以按ID读取数据
XREADGROUP: 按消费组形式读取消息
XPENDING和XACK: XPENDING命令可以用来查询每个消费组内所有消费者已读取但尚未确认的消息, 而XACK命令用于向消息队列确认消息处理已完成
//插入消息队列, *表示自动生成唯一标识, key是repo, value是5
XADD mqstream * repo 5
"1599203861727-0" //返回的1599203861727表示以毫秒为单位的当前服务器时间, 0表示编号第一条消息
读取mqstream队列消息, 从1599203861727-0开始, 没有消息的时候自动阻塞时间100毫秒

XREAD BLOCK 100 STREAMS  mqstream 1599203861727-0
1) 1) "mqstream"
   2) 1) 1) "1599274912765-0"
         2) 1) "repo"
            2) "3"
      2) 1) "1599274925823-0"
         2) 1) "repo"
            2) "2"
      3) 1) "1599274927910-0"
         2) 1) "repo"
            2) "1"
$表示读取最新消息, 这里阻塞10000毫秒(10s), 一直没有消息, 就返回空值

XREAD block 10000 streams mqstream $
(nil)
(10.00s)
消费组消费

创建消费组XGROUP create mqstream group1 0, 组名为group1, 消费mqstream队列
让group1消费组里的消费者consumer1从mqstream中读取所有消息, 命令最后的参数">", 表示从第一条尚未被消费的消息开始读取.
XREADGROUP group group1 consumer1 streams mqstream >
1) 1) "mqstream"
   2) 1) 1) "1599203861727-0"
         2) 1) "repo"
            2) "5"
      2) 1) "1599274912765-0"
         2) 1) "repo"
            2) "3"
      3) 1) "1599274925823-0"
         2) 1) "repo"
            2) "2"
      4) 1) "1599274927910-0"
         2) 1) "repo"
            2) "1"
消费完了其他消费者就不能在消费了
XREADGROUP group group1 consumer2  streams mqstream 0
1) 1) "mqstream"
   2) (empty list or set)
想让消费者负载均衡读取消息
XREADGROUP group group2 consumer1 count 1 streams mqstream >
1) 1) "mqstream"
   2) 1) 1) "1599203861727-0"
         2) 1) "repo"
            2) "5"

XREADGROUP group group2 consumer2 count 1 streams mqstream >
1) 1) "mqstream"
   2) 1) 1) "1599274912765-0"
         2) 1) "repo"
            2) "3"

XREADGROUP group group2 consumer3 count 1 streams mqstream >
1) 1) "mqstream"
   2) 1) 1) "1599274925823-0"
         2) 1) "repo"
            2) "2"
Streams会自动使用内部队列（也称为PENDING List）留存消费组里每个消费者读取的消息，直到消费者使用XACK命令通知 Streams“消息已经处理完成”。如果消费者没有成功处理消息，它就不会给Streams发送XACK命令，消息仍然会留存。此时，消费者可以在重启后，用XPENDING命令查看已读取、但尚未确认处理完成的消息

例如，我们来查看一下 group2 中各个消费者已读取、但尚未确认的消息个数。其中，XPENDING 返回结果的第二、三行分别表示 group2 中所有消费者读取的消息最小 ID 和最大ID

XPENDING mqstream group2
1) (integer) 3
2) "1599203861727-0"
3) "1599274925823-0"
4) 1) 1) "consumer1"
      2) "1"
   2) 1) "consumer2"
      2) "1"
   3) 1) "consumer3"
      2) "1"
如果我们还需要进一步查看某个消费者具体读取了哪些数据，可以执行下面的命令


XPENDING mqstream group2 - + 10 consumer2
1) 1) "1599274912765-0"
   2) "consumer2"
   3) (integer) 513336
   4) (integer) 1
以看到，consumer2 已读取的消息的 ID 是 1599274912765-0

一旦消息 1599274912765-0 被 consumer2 处理了，consumer2 就可以使用 XACK 命令通知 Streams，然后这条消息就会被删除。当我们再使用 XPENDING 命令查看时，就可以看到，consumer2 已经没有已读取、但尚未确认处理的消息了

 XACK mqstream group2 1599274912765-0
(integer) 1
XPENDING mqstream group2 - + 10 consumer2
(empty list or set)
redis变慢优化

避免单线程阻塞

客户端阻塞点

因为多路复用, 一般网络IO不会是阻塞点. 但是因为单线程, 所以复杂度搞得增删改查会阻塞redis.

集合全量查询和聚合操作, 如HGETALL/SMEMBERS等操作和交/并/差集处理
bigKey删除, 因为释放内存操作系统需要把释放的内存块插入一个空闲内存块的链表, 以便后续进行管理和分配
清空数据库, 如FLUSHDB和FLUSHALL
AOF日志同步写, 因为同步写会每一次写入磁盘
主从生成RDB传输给从库使用子线程, 所以不会阻塞主线程. 但是从库收到了RDB文件之后需要FLUSHDB清空数据库, 并且还需要把RDB加载到内存. 就会阻塞
RedisCluster需要进行负载均衡或者有实例增删时, 需要进行迁移. 不过哈希槽信息量不大, 而且迁移是渐进式的, 所以一般阻塞风险不大. 但是如果迁移的是bigkey的话, 就会造成主线程阻塞, 因为迁移是同步迁移
解决:

键值对删除, UNLINK命令(Redis4.0之后, 需要开启lazy-free惰性删除)
清空数据库, FLUSHDB ASYNC 和FLUSHALL AYSNC(Redis4.0之后)
全量查询使用SCAN命令分批读取, 客户端聚合计算
从库加载RDB文件, 建议把主库数据量大小控制在2-4GB左右
为什么CPU结构会影响Redis性能

先说一下主流服务器CPU架构

一个CPU会有多个物理核, 每个物理核都可以运行应用程序

一个物理核拥有一级缓存(Level 1 cache), 包括一级指令缓存和一级数据缓存. 二级缓存, L1Cache和L2Cache只能被自己的物理核使用. 一般都是KB级别

不同的物理核之间会共享一个三级缓存(L3Cache), 一般都是几MB到几十MB, 当L1/L2没有数据缓存时, 可以访问L3, 尽可能避免访问内存

现在主流CPU中一个物理核中会有多个逻辑核, 共享使用L1,L2缓存

- 一个服务器
    - 多个CPU(CPU Socket)
        - 一个CPU多个物理核
            - 一个物理核多个逻辑核
                - 一个逻辑核: L1 和 L2
        - 多个物理核共享一个L3
然而应用程序可以再不同处理器上运行, redis可能在CPU Socket1运行一段时间之后, 然后被调度到Socket2上运行. 被调度到其他CPU之后访问内存需要使用远程内存访问

在多CPU架构下，一个应用程序访问所在Socket的本地内存和访问远端内存的延迟并不一致，所以，我们也把这个架构称为非统一内存访问架构（Non-Uniform Memory Access，NUMA 架构）

所以如果要优化CPU, 就需要将redis绑定在一个物理核上. 因为被调度到其他CPU核上, L1 L2缓存需要重新从L3缓存中获取, 甚至内存中加载

//将redis实例绑定在0号核上
taskset -c 0 ./redis-server
优化网络性能, 需要把网络中断处理程序绑定和redis实例绑定在同一个核上, 就不用跨CPU Socket访问内存了.

这时候绑定哪一个CPU编号就需要注意了. 在CPU的NUMA架构下，对CPU核的编号规则，并不是先把一个CPU Socket中的所有逻辑核编完，再对下一个CPU Socket中的逻辑核编码，而是先给每个CPU Socket中每个物理核的第一个逻辑核依次编号，再给每个CPU Socket中的物理核的第二个逻辑核依次编号.

假设有2个CPU Socket，每个Socket 上有6个物理核，每个物理核又有2个逻辑核，总共24个逻辑核。我们可以执行lscpu命令，查看到这些核的编号：

lscpu

Architecture: x86_64
...
NUMA node0 CPU(s): 0-5,12-17
NUMA node1 CPU(s): 6-11,18-23
...
缺点: 如果绑定在一个逻辑核上, 那么RDB和AOF开启子进程就会和主线程竞争资源

解决方案:

一个redis实例绑定一个物理核taskset -c 0,12 ./redis-server
优化redis源码
排查redis变慢

变慢很可怕, 作为缓存会拖垮数据库, 作为事务会拖慢事务完成等.

是否变慢

使用基于当前环境下的Redis基线性能

举个例子，比如说我们运行下面的命令，该命令会打印120秒内监测到的最大延迟。可以看到，这里的最大延迟是119微秒，也就是基线性能为119微秒。一般情况下，运行120秒就足够监测到最大延迟了，所以我们可以把参数设置为120。

./redis-cli --intrinsic-latency 120
Max latency so far: 17 microseconds.
Max latency so far: 44 microseconds.
Max latency so far: 94 microseconds.
Max latency so far: 110 microseconds.
Max latency so far: 119 microseconds.

36481658 total runs (avg latency: 3.2893 microseconds / 3289.32 nanoseconds per run).
Worst run took 36x longer than the average latency.
(生产环境中,数据量比较大的时候内部延迟超过100也是很正常的，不会对用户的读写性能造成影响)。不过需要注意的是，内部延迟严重依赖于cpu的load，如果你的系统有其他应用在共享cpu，那么对不起，你的内部延迟一定很大。
一般来说，你要把运行时延迟和基线性能进行对比，如果你观察到的Redis运行时延迟是其基线性能的2倍及以上，就可以认定Redis变慢了。

如果你想了解网络对 Redis 性能的影响，一个简单的方法是用 iPerf 这样的工具，测量从 Redis 客户端到服务器端的网络延迟。如果这个延迟有几十毫秒甚至是几百毫秒，就说明，Redis 运行的网络环境中很可能有大流量的其他应用程序在运行，导致网络拥塞了。这个时候，你就需要协调网络运维，调整网络的流量分配了。

慢原因

慢命令. 使用复杂地O(1)最好, redis官方文档有每个命令复杂度. 比如返回SET所有成员不要使用SMEMBERS命令, 而是用SSCAN多次迭代返回, 避免一次返回大量数据. 排序, 交并差集可以在客户端完成. 不能使用KEYS
避免过期时间设置全都一样, 因为大批量一起过期删除会阻塞线程
AOF回盘策略no/everysec/always(回盘策略依赖文件系统的write/fsync两个系统调用完成, write写入内核缓冲区就返回, fsync写入磁盘返回). 尽量不要使用always. aof重写会调用子线程由fsync写入磁盘, 本来不会阻塞, 因为子线程. 但是如果子线程很慢, 主线程又发起了一次重写, 就会阻塞主线程. 如果允许一定数据丢失, 可以no-appendfsync-on-rewrite yes. aof重写不会进行fsync操作. 如果都要就换成固态硬盘吧
内存不够导致出发linux的swap, 建议增加机器内存或使用集群. 查看是否swap
//查看redis进程号
$ redis-cli info | grep process_id
process_id: 5332
//进入/proc下该进程目录中
$ cd /proc/5332
//每一行Size表示的是Redis实例所用的一块内存大小，而Size下方的Swap和它相对应，表示这块Size大小的内存区域有多少已经被换出到磁盘上了。如果这两个值相等，就表示这块内存区域已经完全被换出到磁盘了。
$cat smaps | egrep '^(Swap|Size)'
Size: 584 kB
Swap: 0 kB
Size: 4 kB
Swap: 4 kB
Size: 4 kB
Swap: 0 kB
Size: 462044 kB
Swap: 462008 kB
Size: 21392 kB
Swap: 0 kB
内存大页, Linux内核从2.6.38开始支持内存大页机制,该机制支持2MB大小的内存页分配,而常规的内存页分配是按4KB的粒度来执行的. 虽然内存大页可以减少分配次数, 但是持久化的时候写时复制会拷贝一个内存页. 就算修改100B数据也会拷贝2MB内存大页.
//always表示内存大页启动了, never表示被禁止
cat /sys/kernel/mm/transparent_hugepage/enabled
//禁止内存大页
echo never /sys/kernel/mm/transparent_hugepage/enabled
再加上上面两章一起就是全部阻塞点了

总结:

使用复杂度过高的命令或一次查询全量数据
操作bigkey
大量key集中过期
内存达到maxmemory
客户端使用短连接和Redis相连
当Redis实例的数据量大时，无论是生成RDB，还是AOF重写，都会导致fork耗时严重
AOF的写回策略为always，导致每个操作都要同步刷回磁盘
Redis实例运行机器的内存不足，导致swap发生，Redis需要到swap分区读取数据
进程绑定CPU不合理
Redis实例运行机器上开启了透明内存大页机制
网卡压力过大
检测慢查询

使用慢查询日志

需要设置两个参数:

slowlog-log-slower-than：这个参数表示，慢查询日志对执行时间大于多少微秒的命令进行记录
slowlog-max-len(默认128, 建议1000)：这个参数表示，慢查询日志最多能记录多少条命令记录.
//查看最近一条慢查询日志
SLOWLOG GET 1
1) 1) (integer) 33           //每条日志的唯一ID编号
   2) (integer) 1600990583   //命令执行时的时间戳
   3) (integer) 20906        //命令执行的时长，单位是微秒
   4) 1) "keys"               //具体的执行命令和参数
      2) "abc*"
   5) "127.0.0.1:54793"      //客户端的IP和端口号
   6) ""                     //客户端的名称，此处为空
使用latency monitor监控工具

要使用 latency monitor，首先要设置命令执行时长的阈值

//把latency monitor监控的命令执行时长阈值设为1000微秒
config set latency-monitor-threshold 1000


//查看最新和最大的超过阈值的延迟情况
latency latest
1) 1) "command"
   2) (integer) 1600991500    //命令执行的时间戳
   3) (integer) 2500           //最近的超过阈值的延迟
   4) (integer) 10100          //最大的超过阈值的延迟
排查bigkey

统计bigkey, 这个工具通过扫描数据库来查找, 所以对redis性能会有所影响, 建议在从库使用. 或者加上-i参数控制扫描时间. ./redis-cli --bigkeys -i 0.1表示每扫描100次暂停100毫秒（0.1 秒）

./redis-cli  --bigkeys

-------- summary -------
Sampled 32 keys in the keyspace!
Total key length in bytes is 184 (avg len 5.75)

//统计每种数据类型中元素个数最多的bigkey
Biggest   list found 'product1' has 8 items
Biggest   hash found 'dtemp' has 5 fields
Biggest string found 'page2' has 28 bytes
Biggest stream found 'mqstream' has 4 entries
Biggest    set found 'userid' has 5 members
Biggest   zset found 'device:temperature' has 6 members

//统计每种数据类型的总键值个数，占所有键值个数的比例，以及平均大小
4 lists with 15 items (12.50% of keys, avg size 3.75)
5 hashs with 14 fields (15.62% of keys, avg size 2.80)
10 strings with 68 bytes (31.25% of keys, avg size 6.80)
1 streams with 4 entries (03.12% of keys, avg size 4.00)
7 sets with 19 members (21.88% of keys, avg size 2.71)
5 zsets with 17 members (15.62% of keys, avg size 3.40)
缺点:

只能返回类中类型中最大的那个bigkey, 无法获得排在前N位的
这个只统计集合个数多少, 而不是实际占用内存量.
内存碎片

redis内存分配器默认是jemalloc, 它是按照固定大小划分内存空间, 例如 8 字节、16 字节、32 字节、48 字节，... 2KB、4KB、8KB 等. 例如，Redis 申请一个20字节的空间保存数据，jemalloc就会分配32字节，此时，如果应用还要写入 10 字节的数据, Redis就不用再向操作系统申请空间了，因为刚才分配的32字节已经够用了，这就避免了一次分配操作。 删除修改等会产生碎片.

查看碎片

INFO memory
# Memory
used_memory:1073741736 //实际申请使用的空间
used_memory_human:1024.00M
used_memory_rss:1997159792  //实际分配给Redis的物理内存空间
used_memory_rss_human:1.86G
…
mem_fragmentation_ratio:1.86
mem_fragmentation_ratio范围

大于1但小于1.5: 合理范围, 因为碎片化不可避免
大于1.5: 碎片率已经超过50%, 需要采取一些措施降低内存碎片率
小于1: 这里就表示出发swap, 要重视
解决方法:

重启redis
从4.0-RC3版本后有一个自动清理内存碎的方法, 主线程运行, 有阻塞风险
//开启碎片自动清理
config set activedefrag yes
同时满足两个条件才会自动清理:

active-defrag-ignore-bytes 100mb:表示内存碎片的字节数达到100MB时
active-defrag-threshold-lower 10:表示内存碎片空间占操作系统分配给Redis的总空间比例达到10%时
为了尽可能减少碎片清理对Redis正常请求处理的影响，自动内存碎片清理功能在执行时，还会监控清理操作占用的CPU时间，而且还设置了两个参数，分别用于控制清理操作占用的CPU时间比例的上、下限，既保证清理工作能正常进行，又避免了降低Redis性能。这两个参数具体如下：

active-defrag-cycle-min 25： 表示自动清理过程所用CPU时间的比例不低于25%，保证清理能正常开展；
active-defrag-cycle-max 75：表示自动清理过程所用CPU时间的比例不高于75%，一旦超过，就停止清理，从而避免在清理时，大量的内存拷贝阻塞 Redis，导致响应延迟升高。
redis缓冲区

缓冲区作用是客户端发送操作命令的时候, 就会暂存客户端发送的命令. 这样就不会导致redis来不及处理而丢失数据. 其实除了输入有缓冲区, 输出也是有缓冲区, 先到都是先到缓冲区之后再处理.

缓冲区溢出情况?

写入了bigkey, 比如一下子写入了多个百万级别的集合类型数据
redis处理很慢, 导致缓冲区越堆积越多
查看缓冲区

CLIENT LIST
id=5 addr=127.0.0.1:50487 fd=9 name= age=4 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=26 qbuf-free=32742 obl=0 oll=0 omem=0 events=r cmd=client
缓冲区信息关注点:

客户端与服务器连接的信息, 这里只有一个连接所以只显示了一个IP:端口
cmd: 客户端最新命令; qbuf: 缓冲区已使用大小; qbuf-free: 缓冲区未使用大小, 如果这个值很小就需要注意了
解决输入缓冲区溢出:

避免太多客户端连接redis服务, 在代码中缓冲区上限阈值是1GB, 就是客户端连接占用大于1GB可能崩溃. 这里没有参数去修改
避免写入bigkey, 避免redis主线程阻塞
输出缓冲区分两部分: 大小为16kb的固定缓冲区, 用来暂存OK响应和出错信息; 一个可以动态增加的缓冲区空间, 暂存响应结果 解决输出缓冲区溢出:

服务器端返回bigkey的大量结果
MONITOR命令用来检测redis, 会持续检测各个命令操作. 但是也会持续占用输出缓冲区. 只在调试环境使用, 不要在生产使用(偶尔使用没问题).
设置缓冲区大小.
普通客户端:

//normal 表示当前设置的是普通客户端, 0分别是缓冲区大小限制/缓冲区持续写入量限制/持续写入时间限制, 就是都不限制
client-output-buffer-limit normal 0 0 0
订阅了Redis频道的订阅客户端, 一旦订阅的 Redis 频道有消息了，服务器端都会通过输出缓冲区把消息发给客户端。所以，订阅客户端和服务器间的消息发送方式，不属于阻塞式发送。不过，如果频道消息较多的话，也会占用较多的输出缓冲区空间

//pubsub表示订阅客户端, 输出缓冲区上限8MB, 超过关闭客户端连接; 连续60s对输出缓冲区写入超过2MB关闭客户端连接
client-output-buffer-limit pubsub 8mb 2mb 60
主从使用缓冲区

TODO

缓存淘汰机制

先我们要知道如何设置redis最大缓存容量CONFIG SET maxmemory 4gb

8中淘汰策略:

不进行数据淘汰 noeviction(3.0之后默认): 缓存写满之后直接报错
在设置过期时间数据中淘汰
volatile-random: 设置了过期时间的随机删除
volatile-ttl: 过期时间越早的优先删除
volatile-lru(3.0之前默认): LRU算法删除
volatile-lfu: LFU算法删除
所有数据中淘汰
allkeys-random: 所有键中随机删除
allkeys-lru: 所有键中LRU算法删除
allkeys-lfu: 所有键中LFU算法删除
一般LRU算法都是使用链表, 使用过的就被插入到head. 缓存满了从tail开始删除. 但是这样需要用链表管理所有redis数据, 而且会带来很多链表移动操作, 会降低redis性能. 所以redis中是在redisObject中lru字段记录最近使用的时间戳. 淘汰的时候会随机选出N个数据作为候选集合, 然后把lru值最小的淘汰. 之后再次淘汰数据, redis需要挑选进入候选集合的lru必须小于之前候选集合的最小值

TODO 具体如何删除过期键

缓存和数据库不一致

只读缓存: 先读取缓存中有没有, 有直接返回. 如果数据库更新了数据, 删除缓存. 下一次读取的时候没有数据, 先查询数据库, 并写入缓存中 读写缓存: 同时修改数据库和缓存中的值.

同步直写策略：写缓存时，也同步写数据库，缓存和数据库中的数据一致
异步写回策略：写缓存时不同步写数据库，等到数据从缓存中淘汰时，再写回数据库。使用这种策略时，如果数据还没有写回数据库，缓存就发生了故障，那么，此时，数据库就没有最新的数据了
缓存中有数据, 和数据库一致/缓冲中没有数据: 除了这种情况外其他都是缓冲和数据库不一致

读写数据不一致解决: 删除缓存值或更新数据库失败而导致数据不一致, 你可以使用重试机制确保删除或更新操作成功

只读缓存有什么数据不一致的情况.

先删除缓存, 在更新数据库. 这时候A删除缓存后, 还没来得及更新数据库. 然后B读取缓存, 发现没有缓存有查询了旧数据之后更新成缓存, 最后A才更新数据库. 这样缓存就是旧数据

延迟双删

//先删除
redis.delKey(X)
//假设B这时候查询缓存没有数据, 然后去查询数据在放入缓存中
db.update(X)
//这里延迟就要是为了B可以查询数据然后放入缓存的时间, 只要放入了缓存, 之后再删除, 数据就是一致的
Thread.sleep(N)
redis.delKey(X)
缺点: 如果业务应用中读取数据库和写缓存的时间不好估算，那么延迟双删中的等待时间就不好设置

先更新数据库, 再删除缓存. A更新了数据库, 但是没有来得及删除缓存. B开始读取缓存, 就会读取到旧缓存. 其实一般删除缓存很快, 而且就算读取旧缓存也就是当时并发的B会读取, 其他的还是会读取到新缓存. 所以这种对业务影响比较小. 如果删除失败, 就消息队列重试删除

缓存雪崩/击穿/穿透

雪崩: 大量缓存同时过期或者redis宕机 解决: 过期时间加一个小的随机值, 服务降级/主从集群

击穿: 热点数据过期, 并发很大导致很多请求都进入数据库. 热点数据不设置过期时间

穿透: 本身就没有这个数据, 然后每次请求缓存没有, 在请求到数据库也没有. 下一次还是请求到数据库. 解决: 缓存空值或缺省值/使用布隆过滤器/对于恶意请求的拦截

布隆过滤器

一个很长的bit[]组成, 默认都是保存0. 然后来了一个值, 经过N次hash运算, 每次计算出的hash都放入对应位置并把值改为1. 比如现在进行三次hash计算, 分别放入了位置1,100,1000的位置, 值都是1. 然后我们判断的时候只需要hash N次, 看看位置都是1. 如果有一个位置为0, 表示肯定不存在. 如果全部都是1, 表示可能存在, 因为可能是巧合别的hash全部改为了1

缓存污染

用的很少的缓存还是一直存在, 直到内存满了之后自动淘汰掉, 但是这样引入额外的时间开销, 影响性能.

LRU(Least Recently Used)最近最少使用, 首先淘汰最长时间未被使用

LFU(Least Frequently Used)最近不经常使用, 首先淘汰一定时期内被访问次数最少

解决缓存污染:可以使用LFU

Pika

如果数据量很大, redis成本很高的话, 可以选择Pika. 基于SSD给Redis单实例进行扩容的技术方案

无锁原子操作

如果库存减一, 先读取库存, 在减一, 写入库存缓存

解决:

使用多个操作实现的一个操作(单命令操作), 比如INCR/DECR命令
使用Lua脚本
分布式锁

可以试试Etcd

单机式: https://github.com/JingRui-H/blog/blob/master/%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/4.redis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81.md

优化删除可以用lua脚本先判断value是否一致, 再删除.

redlock算法

为了避免 Redis 实例故障而导致的锁无法工作的问题，Redis 的开发者 Antirez 提出了分布式锁算法 Redlock. 是让客户端和多个独立的 Redis 实例依次请求加锁，如果客户端能够和半数以上的实例成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁了，否则加锁失败

事务

命令入队时就报错，会放弃事务执行，保证原子性
命令入队时没报错，实际执行时报错，不保证原子性
EXEC命令执行时实例故障，如果开启了AOF日志，可以保证原子性。
主从可能的坑

主从数据不一致

尽量保证网络快速, 主从库部署在同一个机房

开发一个外部程序来监控主从库间的复制进度。因为 Redis 的 INFO replication 命令可以查看主库接收写命令的进度信息（master_repl_offset）和从库复制写命令的进度信息（slave_repl_offset），所以，我们就可以开发一个监控程序，先用 INFO replication 命令查到主、从库的进度，然后，我们用 master_repl_offset 减去 slave_repl_offset，这样就能得到从库和主库间的复制进度差值了

读取到过期信息

过期删除策略:

惰性删除
定期删除(默认100ms随机选出一定数量的数据)
如果从主库读取过期信息, 触发惰性删除. 但是从库本身不会执行删除操作. 在redis3.2版本之前, 会直接返回过期数据, redis3.2版本之后在从库上获取过期数据的时候虽然从库不会删除, 但是会返回空值.

如果设置过期时间因为主从同步被延后了也可能获取到过期数据, 这种情况建议使用EXPIREAT/PEXPIREAT命令, 使用具体的时间点过期

主从不合理配置导致服务器挂掉

protected-mode: 作用是限定哨兵实例能否被其他服务器访问, yes只能部署服务器本地访问; no其他服务器也可以访问

如果哨兵部署在不同服务器并且设置为yes, 哨兵之间就无法通信.

//设置为no, 并且bind其他哨兵的ip, 就可以保证安全性
protected-mode no
bind 192.168.10.3 192.168.10.4 192.168.10.5
cluster-node-timeout: Redis Cluster集群中为每个实例配置了一主一从模式时, 如果主从切换比较慢超过了这个超时时间, 就会导致这个节点挂掉. 所以建议这个时间配置大一点(比如10-20秒)

脑裂

主从集群有1个主库, 5个从库, 3个哨兵. 脑裂就是同时有两个主节点, 然后客户端往不同的主节点写入数据, 导致数据丢失.

原主库假故障导致脑裂, 假设原主库只是暂时异常无法响应哨兵心跳, 哨兵把主库判断为客观下线, 开始主从切换. 然后原主库恢复正常, 连接的客户端插入数据, 但是有一个从库又变成了主库. 然后哨兵就会让原主库执行slave of命令, 和新主库进行全量同步. 原主库会清空本地数据, 这时候主从切换期间保存的数据就丢失了

min-slaves-to-write：这个配置项设置了主库能进行数据同步的最少从库数量
min-slaves-max-lag：这个配置项设置了主从库间进行数据复制时，从库给主库发送 ACK 消息的最大延迟（以秒为单位）。
假设我们将 min-slaves-to-write 设置为 1，把 min-slaves-max-lag 设置为 12s，把哨兵的 down-after-milliseconds 设置为 10s，主库因为某些原因卡住了 15s，导致哨兵判断主库客观下线，开始进行主从切换。同时，因为原主库卡住了 15s，没有一个从库能和原主库在 12s 内进行数据复制，原主库也无法接收客户端请求了。这样一来，主从切换完成后，也只有新主库能接收请求，不会发生脑裂，也就不会发生数据丢失的问题了。

我给你的建议是，假设从库有K个，可以将 min-slaves-to-write 设置为 K/2+1（如果K等于1，就设为 1），将 min-slaves-max-lag 设置为十几秒（例如10～20s），在这个配置下，如果有一半以上的从库和主库进行的ACK消息延迟超过十几秒，我们就禁止主库接收客户端写请求。

codis VS redis cluster

codis server: 这是进行了二次开发的Redis实例，其中增加了额外的数据结构，支持数据迁移操作，主要负责处理具体的数据读写请求。
codis proxy：接收客户端请求，并把请求转发给codis server。
Zookeeper集群：保存集群元数据，例如数据位置信息和codis proxy信息。
codis dashboard 和 codis fe：共同组成了集群管理工具。其中，codis dashboard 负责执行集群管理工作，包括增删codis server、codis proxy和进行数据迁移。而codis fe负责提供dashboard的Web操作界面，便于我们直接在Web界面上进行集群管理。
gossip协议

Cluster中的每个节点都维护一份在自己看来当前整个集群的状态，主要包括：

当前集群状态
集群中各节点所负责的slots信息，及其migrate状态
集群中各节点的master-slave状态
集群中各节点的存活状态及不可达投票
基于Gossip协议当集群状态变化时，如新节点加入、slot迁移、节点宕机、slave提升为新Master，我们希望这些变化尽快的被发现，传播到整个集群的所有节点并达成一致。节点之间相互的心跳（PING，PONG，MEET）及其携带的数据是集群状态传播最主要的途径。

Redis 集群是去中心化的，彼此之间状态同步靠 gossip 协议通信，集群的消息有以下几种类型：

Meet 通过「cluster meet ip port」命令，已有集群的节点会向新的节点发送邀请，加入现有集群。
Ping 节点每秒会向集群中其他节点发送 ping 消息，消息中带有自己已知的两个节点的地址、槽、状态信息、最后一次通信时间等。
Pong 节点收到 ping 消息后会回复 pong 消息，消息中同样带有自己已知的两个节点信息。
Fail 节点 ping 不通某节点后，会向集群所有节点广播该节点挂掉的消息。其他节点收到消息后标记已下线。
由于 gossip 协议对服务器时间的要求较高，否则时间戳不准确会影响节点判断消息的有效性。另外节点数量增多后的网络开销也会对服务器产生压力，同时结点数太多，意味着达到最终一致性的时间也相对变长，因此官方推荐最大节点数为1000左右

基于Gossip协议的故障检测

集群中的每个节点都会定期地向集群中的其他节点发送PING消息，以此交换各个节点状态信息，检测各个节点状态：在线状态、疑似下线状态PFAIL、已下线状态FAIL。

自己保存信息：当主节点A通过消息得知主节点B认为主节点D进入了疑似下线(PFAIL)状态时,主节点A会在自己的clusterState.nodes字典中找到主节点D所对应的clusterNode结构，并将主节点B的下线报告添加到clusterNode结构的fail_reports链表中，并后续关于结点D疑似下线的状态通过Gossip协议通知其他节点。
一起裁定：如果集群里面，半数以上的主节点都将主节点D报告为疑似下线，那么主节点D将被标记为已下线(FAIL)状态，将主节点D标记为已下线的节点会向集群广播主节点D的FAIL消息，所有收到FAIL消息的节点都会立即更新nodes里面主节点D状态标记为已下线。
最终裁定：将 node 标记为 FAIL 需要满足以下两个条件：有半数以上的主节点将 node 标记为 PFAIL 状态。当前节点也将 node 标记为 PFAIL 状态。
也就是说当前节点发现其他结点疑似挂掉了，那么就写在自己的小本本上，等着通知给其他好基友，让他们自己也看看，最后又一半以上的好基友都认为那个节点挂了，并且那个节点自己也认为自己挂了，那么就是真的挂了，过程还是比较严谨的。

redis6.0新特性

redis6.0多线程: 之前虽然AOF重写/快照生成/异步删除等都是多线程处理. 但是从网络IO处理到实际读写命令处理都是单线程处理的. 现在网络IO处理也改为多线程处理, 而实际读写命令处理依旧是单线程.

redis6.0主线程和IO线程协作:

首先，主线程负责接收建立连接请求。当有客户端请求和实例建立 Socket 连接时，主线程会创建和客户端的连接，并把 Socket 放入全局等待队列中。紧接着，主线程通过轮询方法把 Socket 连接分配给 IO 线程。
主线程一旦把 Socket 分配给 IO 线程，就会进入阻塞状态，等待 IO 线程完成客户端请求读取和解析。因为有多个 IO 线程在并行处理，所以，这个过程很快就可以完成。
等到 IO 线程解析完请求，主线程还是会以单线程的方式执行这些命令操作
当主线程执行完请求操作后，会把需要返回的结果写入缓冲区，然后，主线程会阻塞等待 IO 线程把这些结果回写到 Socket 中，并返回给客户端。
`` //表示启用多线程, 默认关闭的 io-threads-do-reads yes //线程数小于redis的cpu个数, 比如8核建议配置6个IO线程 io-threads 6

如果你在实际应用中，发现Redis实例的CPU开销不大，吞吐量却没有提升，可以考虑使用Redis 6.0的多线程机制，加速网络处理，进而提升实例的吞吐量。
© 2021 GitHub, Inc.
Terms
Privacy
Security
Status
Docs
Contact GitHub
Pricing
API
Training
Blog
About
